<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="This paper introduces Find n' Propagate, a novel method for open-vocabulary 3D object detection in urban environments, tackling limitations of existing systems by improving recall and detection of novel object classes using vision-language models (VLMs) and multi-sensor data.">
  <meta property="og:title" content="Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments"/>
  <meta property="og:description" content="Find n' Propagate is a novel method that addresses the limitations of current LiDAR-based 3D object detection systems by increasing recall for novel objects through a greedy box seeker and remote simulator."/>
  <meta property="og:url" content="https://github.com/djamahl99/findnpropagate"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="Find n' Propagate: Open-Vocabulary 3D Object Detection"/>
  <meta name="twitter:description" content="Find n' Propagate leverages vision-language models and multi-sensor data to maximize recall for novel objects in urban environments, with a 53% improvement in novel recall."/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimensions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="3D Object Detection, Open-Vocabulary Learning, LiDAR, Vision-Language Models, Urban Environments, Novel Object Detection, Self-Training, Greedy Box Seeker, Remote Simulator">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <meta name="robots" content="index, follow">
  <title>Find n' Propagate | ECCV 2024</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    p {
        margin-bottom: 1.5em;
        line-height: 1.6;
    }

    .has-text-justified {
        text-align: left;
    }    

    @media (max-width: 768px) {
        .carousel .item {
            width: 100%;
        }
    }
    
  </style>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
  
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!-- <h1></h1> -->

              <div class="author-list">
                <span class="author-block">
                  <a href="https://yourpersonalurl1.com" target="_blank">Djamahl Etchegaray</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://yourpersonalurl2.com" target="_blank">Zi Huang</a><sup>1</sup>,
                </span>
                <span class="author-block">
                  <a href="https://yourpersonalurl3.com" target="_blank">Tatsuya Harada</a><sup>2</sup>,
                </span>
                <span class="author-block">
                  <a href="https://yourpersonalurl4.com" target="_blank">Yadan Luo</a><sup>1</sup>
                </span>
              </div>
              
              <p style="margin-bottom:0"><sup>1</sup>UQMM Lab, University of Queensland, Brisbane, Australia</p>
              <p style="margin-bottom:0"><sup>2</sup>The University of Tokyo, Tokyo, Japan</p>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">ECCV 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/djamahl99/findnpropagate" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2403.13556" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>In this work, we tackle the limitations of current LiDAR-based 3D object detection systems, which are hindered by a restricted class vocabulary and the high costs associated with annotating new object classes. Our exploration of open-vocabulary (OV) learning in urban environments aims to capture novel instances using pre-trained vision-language models (VLMs) with multi-sensor data. We design and benchmark a set of four potential solutions as baselines, categorizing them into either top-down or bottom-up approaches based on their input data strategies.</p>

          <p>While effective, these methods exhibit certain limitations, such as missing novel objects in 3D box estimation or applying rigorous priors, leading to biases towards objects near the camera or of rectangular geometries. To overcome these limitations, we introduce a universal <strong>Find n' Propagate</strong> approach for 3D OV tasks, aimed at maximizing the recall of novel objects and propagating this detection capability to more distant areas thereby progressively capturing more.</p>
      
          <p>In particular, we utilize a greedy box seeker to search against 3D novel boxes of varying orientations and depth in each generated frustum and ensure the reliability of newly identified boxes by cross alignment and density ranker. Additionally, the inherent bias towards camera-proximal objects is alleviated by the proposed remote simulator, which randomly diversifies pseudo-labeled novel instances in the self-training process, combined with the fusion of base samples in the memory bank.</p>
      
          <p>Extensive experiments demonstrate a 53% improvement in novel recall across diverse OV settings, VLMs, and 3D detectors. Notably, we achieve up to a 3.97-fold increase in Average Precision (AP) for novel object classes. The source code is made available at <a href="https://github.com/djamahl99/findnpropagate" target="_blank">github.com/djamahl99/findnpropagate</a>.</p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Baselines section -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Baselines</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/tp-bu-eccv.jpg" style="width: 75%; min-width:640px;border: none;">
          <!-- <iframe src="images/tp-bu-eccv-new.pdf" style="width: 100%;height: 100%;border: none;"></iframe> -->
        </div>
      </div>
      <p>In this work, we investigate the potential of leveraging OV learning for 3D object detection by employing high-resolution LiDAR data (<strong>Top</strong>) and multi-view imagery (<strong>Bottom</strong>). As illustrated in Fig. <a href="#fig-top-down-bottom-up">1</a>, four baseline solutions are designed: (1) <strong>Top-down Projection</strong>, (2) <strong>Top-down Self-train</strong>, (3) <strong>Top-down Clustering</strong>, and (4) <strong>Bottom-up Weakly-supervised</strong> 3D detection approaches to facilitate novel object discovery in point clouds.</p>

      <p>The foundation of our <strong>Top-down</strong> strategies is inspired by advancements in 2D OV learning, where one can regress class-agnostic bounding boxes based on base box annotations and subsequently leverage VLMs for open-vocabulary classification. Based on that, the <strong>Top-down Self-train</strong> is the variant that further enhances open-vocabulary performance through self-training mechanisms. Beyond mere 2D projections, our third <strong>Top-down</strong> baseline explores the feasibility of applying open-vocabulary 3D segmentation directly to 3D detection tasks, utilizing clustering techniques for 3D bounding box estimation.</p>
  
      <p>Nevertheless, it is observed that <strong>Top-down</strong> methods can easily overfit to known classes, potentially <em>overlooking</em> novel objects with varying sizes and shapes. As shown in the visualization of Fig. <a href="#fig-top-down-bottom-up">1</a>, unseen objects that are of vastly different shapes, such as long vehicles like buses or small traffic cones, often go undetected in class-agnostic 3D proposals and are obscured in 2D crops due to occlusion.</p>
  
      <p>The <strong>Bottom-up</strong> approach presents a cost-effective alternative akin to weakly-supervised 3D object detection, lifting 2D annotations to construct 3D bounding boxes. Different from <strong>Top-down</strong> counterparts, this approach is training-free and does not rely on any base annotations, potentially making it more generalizable and capable of finding objects with diverse shapes and densities. In Baseline IV, we study FGR <a href="#ref1">(Wei et al., 2021)</a> as an exemplar of <strong>Bottom-up Weakly-supervised</strong> and evaluate its effectiveness in generating novel proposals. FGR starts with removing background points such as the ground plane, then incorporates the human prior into key-vertex localization to refine box regression. However, their study was limited to regressing car objects, as their vertex localization assumes <em>rectangular</em> objects which do not hold for other classes (e.g., pedestrians).</p>
    </div>
  </div>
</section>
<!-- Baselines section -->

<!-- our method section -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Our Method</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <img src="static/images/new_fig_eccv_export.png" style="width: 75%; min-width:640px;border: none;">
          <!-- <iframe src="images/tp-bu-eccv-new.pdf" style="width: 100%;height: 100%;border: none;"></iframe> -->
        </div>
      </div>
      <p>To address the baseline limitations, we propose a novel <strong><em>Find n' Propagate</em></strong> approach to maximize the recall rate of novel objects and then propagate the knowledge to distant regions from the camera progressively. We identify most detection failures of novel objects stem from the uncertainties in 3D object orientation and depth. This observation motivates the development of a <strong>Greedy Box Seeker</strong> strategy that initiates by generating instance frustums for each unique 2D box prediction region, utilizing <strong>Region VLMs</strong> such as GLIP <a href="#ref2">(Li et al., 2022)</a>, or pre-trained OV 2D models like OWL-ViT <a href="#ref3">(Minderer et al., 2022)</a>.</p>
  
      <p>These frustums are segmented into subspaces across different angles and depth levels to facilitate an exhaustive greedy search for the most apt 3D proposal, accommodating a wide variety of shapes and sizes. To control the quality of newly generated boxes, we implement a <strong>Greedy Box Oracle</strong> that employs two key criteria of multi-view alignment and density ranking to select the most probable proposal. The rationale behind that is that 2D predictions predominantly originate from objects near the camera, characterized by dense point clouds and substantial overlap with the 2D box upon re-projection.</p>
  
      <p>Recognizing that relying solely on pseudo labels generated from these 2D predictions could bias the detector towards objects near the camera and overlook those that are distant or obscured, we propose a <strong>Remote Propagator</strong> to mitigate the bias. To augment novel pseudo labels with distant object geometries, geometry and density simulators are employed to perturb pseudo label boxes to farther distances from the camera and mimic sparser structures. The refined 3D proposals are subsequently integrated into a memory bank, facilitating iterative training of the detection model.</p>
  
    </div>
  </div>
</section>
<!-- our method section -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Qualitative Results</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/scene_409_cam_web.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <source src="static/videos/scene_262_cam_web.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <!-- Your video file here -->
            <source src="static/videos/scene_449_cam_web.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>

      <!-- description -->
      <p>
        <strong style="color: green;">Green</strong> boxes represent ground-truth known classes, 
        <strong style="color: pink;">pink</strong> boxes denote ground-truth novel classes, and 
        <span style="color: blue;">blue</span> boxes indicate model predictions.
      </p>      
      
    </div>
  </div>
</section>
<!-- End video carousel -->

<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/ECCV_2024_Find_n_Propagate_Poster.pdf" width="100%" height="550">
      </iframe>
      
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{DBLP:conf/eccv/Etche24,
        author       = {Djamahl Etchegaray and
                        Zi Huang and
                        Tatsuya Harada and
                        Yadan Luo},
        title        = {Find n' Propagate: Open-Vocabulary 3D Object Detection in Urban Environments},
        booktitle    = {Computer Vision - {ECCV} 2024 - The 18th European Conference on Computer Vision},
        year         = {2024},
        url          = {https://doi.org/10.48550/arXiv.2403.13556}
            
      }
  </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
